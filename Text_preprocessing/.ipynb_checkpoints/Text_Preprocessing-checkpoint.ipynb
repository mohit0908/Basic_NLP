{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import re, string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk.corpus import stopwords\n",
    "import krovetzstemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Text: \n",
      "\n",
      " <h1>Title Goes Here</h1>\n",
      "<b>Bolded Text</b>\n",
      "<i>Italicized Text</i>\n",
      "<img src=\"this should all be gone\"/>\n",
      "<a href=\"this will be gone, too\">But this will still be here!</a>\n",
      "I run. He ran. She is running. Will they stop running?\n",
      "I talked. She was talking. They talked to them about running. Who ran to the talking runner?\n",
      "[Some text we don't want to keep is in here]\n",
      "¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\n",
      "something... is! wrong() with.,; this :: sentence.\n",
      "I can't do this anymore. I didn't know them. Why couldn't you have dinner at the restaurant?\n",
      "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
      "Don't do it.... Just don't. Billy! I know what you're doing. This is a great little house you've got here.\n",
      "[This is some other unwanted text]\n",
      "John: \"Well, well, well.\"\n",
      "James: \"There, there. There, there.\"\n",
      "&nbsp;&nbsp;\n",
      "There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\n",
      "I have to go get 2 tutus from 2 different stores, too.\n",
      "22    45   1067   445\n",
      "{{Here is some stuff inside of double curly braces.}}\n",
      "{Here is more stuff in single curly braces.}\n",
      "[DELETE]\n",
      "</body>\n",
      "</html> \n",
      "\n",
      "\n",
      "Processed text: \n",
      "\n",
      " title go bold text italicize text still run ran running stop running talk talk talk running ran talk runner sebastin nicol alejandro jronimo go store tomorrow morning something wrong sentence anymore know could dinner restaurant favorite movie franchise order indiana jones marvel cinematic universe star war back future harry potter billy know great little house got john well well well james lot reason one hundred and one reason one million reason actually go get two tutu two different store twenty-two forty-five one thousand and sixty-seven four hundred and forty-five stuff inside double curly brace stuff single curly brace\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Following preprocessing is performed on html file. It can be a text file also. \n",
    "Just modify file_read function according to data to be read\n",
    "\n",
    "1. Strip html tags and get raw text\n",
    "2. Remove data between square brackets\n",
    "3. Expand contracted words\n",
    "4. Tokenize sentences into words\n",
    "5. Remove any non-ascii character\n",
    "6. Lower case complete corpus\n",
    "7. Remove punctuations\n",
    "8. Replace numbers with word equivalents\n",
    "9. Remove stopwords using predefined list of words in english language\n",
    "10.Stem words. Krovetz stemmer is used. Other options are available as well\n",
    "11.Lemmatize words\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def read_file(filename):\n",
    "    html_file = filename\n",
    "    file = codecs.open(html_file, 'r', 'utf-8').read()\n",
    "    return file\n",
    "\n",
    "def strip_html(text):\n",
    "    file_data = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    return file_data\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\n",
    "    E.g can't --> cannot\n",
    "    didn't --> did not\n",
    "    \"\"\"\n",
    "    \n",
    "    return contractions.fix(text)\n",
    "\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    text_list = []    \n",
    "    for word in text:\n",
    "        text_list.append(''.join([i if ord(i) < 128 else '' for i in word]))\n",
    "    return text_list\n",
    "\n",
    "def to_lowercase(text):\n",
    "    text_list = [word.lower() for word in text]\n",
    "    return text_list\n",
    "\n",
    "def remove_punc(text):\n",
    "    word_list = []\n",
    "    \n",
    "    for word in text:\n",
    "        # Regex needs to be changed as per input text\n",
    "        word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if word != '':\n",
    "            word_list.append(word)\n",
    "    return word_list\n",
    "\n",
    "def replace_numbers(text):\n",
    "    word_list = []\n",
    "    p = inflect.engine()\n",
    "    \n",
    "    for word in text:\n",
    "        if word.isdigit():\n",
    "            word = p.number_to_words(word)\n",
    "        else:\n",
    "            word = word\n",
    "        word_list.append(word)\n",
    "    return word_list\n",
    " \n",
    "def remove_stopwords(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    word_list = [word for word in text if word not in stop]\n",
    "    return word_list\n",
    "\n",
    "def stemmer(text):\n",
    "    # Used krovetz stemmer. Other stemmers can be used\n",
    "    stemmer = krovetzstemmer.Stemmer()\n",
    "    word_list = [stemmer.stem(word) for word in text]\n",
    "    return word_list\n",
    "    \n",
    "def lemmatizer(text):\n",
    "    lemm = WordNetLemmatizer()\n",
    "    word_list = [lemm.lemmatize(word) for word in text]\n",
    "    return word_list\n",
    "\n",
    "def token_normalizer(text):\n",
    "    text = stemmer(text)\n",
    "    text = lemmatizer(text)\n",
    "    return text\n",
    "  \n",
    "def denoise_text(text):\n",
    "    print('Actual Text:','\\n\\n',text, '\\n\\n')\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = replace_contractions(text)\n",
    "    text = tokenize(text)\n",
    "    text = remove_non_ascii(text)\n",
    "    text = to_lowercase(text)\n",
    "    text = remove_punc(text)\n",
    "    text = replace_numbers(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = token_normalizer(text)\n",
    "    print('Processed text:','\\n\\n',' '.join(text))\n",
    "    return text\n",
    "\n",
    "\n",
    "file = read_file('file.html')\n",
    "text = denoise_text(file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
